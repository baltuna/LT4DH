{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LT4DH:ELTeC_NER_en.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baltuna/LT4DH/blob/main/LT4DH_ELTeC_NER_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9YAgkI8Naco"
      },
      "source": [
        "# Extracting part of speech from ELTeC-ENG\n",
        "\n",
        "Adaptation of a great Colab by Borja Navarro for the LT4DH course in the University of the Basque Country.\n",
        "\n",
        "This version (to be cleaned) uses English resources in contrast to the Spanish one used by Borja Navarro.\n",
        "\n",
        "Original reference here:\n",
        "\n",
        "Borja Navarro Colorado | University of Alicante\n",
        "\n",
        "In this case, the information about part of speech has not been manually annotated in the corpus. It is necessary first analyze the novels with a NLP system and then extract the linguistic information. The NLP system used is [SpaCy](https://spacy.io/).\n",
        "\n",
        "The notebook shows:\n",
        "\n",
        "- how to open a novel from ELTeC in COLAB,\n",
        "- how to activate SpaCy in COLAB,\n",
        "- how to analyze the novel with SpaCy, and\n",
        "- how to extract information about Part of Speec.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2j6tERTNTcP"
      },
      "source": [
        "## Loading ELTeC-ENG corpus in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kejn4RdANZEy",
        "outputId": "0689cbb7-bf5c-4740-a76e-05017921d898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget \"https://github.com/COST-ELTeC/ELTeC-eng/archive/refs/heads/master.zip\" # paste here corpus url\n",
        "\n",
        "zip_ref = zipfile.ZipFile('master.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts files here (/content/)\n",
        "zip_ref.close() \n",
        "!rm master.zip #Removes ZIP to save space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-15 17:09:13--  https://github.com/COST-ELTeC/ELTeC-eng/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/COST-ELTeC/ELTeC-eng/zip/refs/heads/master [following]\n",
            "--2022-03-15 17:09:13--  https://codeload.github.com/COST-ELTeC/ELTeC-eng/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.255.120\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.255.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [      <=>           ]  86.87M  4.05MB/s    in 22s     \n",
            "\n",
            "2022-03-15 17:09:35 (4.01 MB/s) - ‘master.zip’ saved [91092993]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLkW02KoOIX6"
      },
      "source": [
        "## SpaCy: download and installing\n",
        "\n",
        "[SpaCy](https://spacy.io/) is a NLP system. It analyzes part of speech and lemmas, sintax (dependencies) and named entities. \n",
        "\n",
        "Three steps:\n",
        "\n",
        "1. Import SpaCy to Colab\n",
        "2. Download language module (English)\n",
        "3. Activate module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHMbKkERXhI",
        "outputId": "6828e157-d8bb-4612-aeb3-2090be6cc753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm #Download Spanish module (the \"small\" module in this case: \"sm\").\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp_eng = en_core_web_sm.load() #Load Spanish analyzer in \"nlp_esp\"."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsk-hIbqRuw5"
      },
      "source": [
        "## Analyzing a novel from ELTeC-ENG\n",
        "\n",
        "Once we have downloaded the corpus and activated SpaCy, let's analyze one novel.\n",
        "\n",
        "First, select from the corpus [ELTeC-ENG](https://github.com/COST-ELTeC/ELTeC-eng/tree/master/level1) a novel and copy the file name. Then paste the name in the variable \"novela_name\". In this example, we will analyze the novel [*Yeast*](https://github.com/COST-ELTeC/ELTeC-eng/tree/master/level1/ENG18510_Kingsley.xml): ENG18510_Kingsley.xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVJtGl9vRzpq",
        "outputId": "5e0227fa-a907-48a0-8672-74da0d9fd0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "novela_name = \"ENG18510_Kingsley.xml\" # Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "#print('Analyzing PoS and lemmas')\n",
        "analisis = nlp_eng(novela_text) #Here the novel is analyzed with SpaCy. All the analysis is stored in \"analisis\" variable.\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing ENG18510_Kingsley.xml\n",
            "Opening the file and extracting text\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_13VKGg9A7Z"
      },
      "source": [
        "Now all the analysis is stored in \"analisis\" variable. It only remains to iterate over the variable and extract the information: in this case, named entities. How to extract information about syntax, named entities, etc. see [SpaCy 101](https://spacy.io/usage/spacy-101)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Zqf9ZC6-0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c6f0a5-7dd1-4919-b1fa-b4c09deeb075"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "def show_ents(doc): #funzionatzen du\n",
        "  if doc.ents: \n",
        "    for ent in doc.ents: \n",
        "     # print(ent.text+' - ' +str(ent.start_char) +' - '+ str(ent.end_char) +' - '+ent.label_) \n",
        "      print(ent.text+'\\t'+str(ent.start_char) +'\\t'+ str(ent.end_char) +'\\t'+ent.label_) \n",
        "    else: print(str(len(doc.ents))+ ' were found')\n",
        "\n",
        "def one_ent(doc): #funzionatzen du\n",
        "  if doc.ents: \n",
        "    for ent in doc.ents: \n",
        "      if ent.label_ == 'LOC':\n",
        "        entitateak.append(ent)\n",
        "        print(ent.text+'\\t'+str(ent.start_char) +'\\t'+ str(ent.end_char) +'\\t'+ent.label_)\n",
        "    else: print(str(len(entitateak))+ ' entities were found')\n",
        "\n",
        "def two_ents(doc): #funzionatzen du\n",
        "  if doc.ents: \n",
        "    for ent in doc.ents: \n",
        "      if ent.label_ == 'TIME' or ent.label_ == 'DATE':\n",
        "        entitateak.append(ent)\n",
        "        print(ent.text+'\\t'+str(ent.start_char) +'\\t'+ str(ent.end_char) +'\\t'+ent.label_)\n",
        "    else: print(str(len(entitateak))+ ' temporal entities were found')\n",
        "\n",
        "\n",
        "\n",
        "entitateak=[]\n",
        "\n",
        "########################ENTITY TYPES#############################\n",
        "# CARDINAL, PERSON, ORG, GPE, NORP, EVENT, DATE, WORK_OF_ART, TIME, PRODUCT, LOC LANGUAGE, FAC, ORDINAL\n",
        "\n",
        "\n",
        "#################################PROGRAMA#########################################\n",
        "\n",
        "#show_ents(analisis)\n",
        "\n",
        "one_ent(analisis)\n",
        "\n",
        "#two_ents(analisis)\n",
        "\n",
        "\n",
        "#entitateak_out = ''\n",
        "#for entitate in entitateak:\n",
        "#  entitateak.sort()\n",
        "#  entity = entitate[0]\n",
        "#  enttype = entitate[3]\n",
        "#  entitateak_out+=str(entity)+'\\t'+enttype+'\\n'\n",
        "\n",
        "\n",
        "#out = open('chosen_entities.csv', 'w') #Opens a file in write mode (\"w\").\n",
        "#out.write(entitateak) # \"Writes\" the content of authors_titles_out in the file\n",
        "#out.close() #Closes the file\n",
        "#files.download('chosen_entities.csv')\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "#num_pers = str(len(pers))\n",
        "\n",
        "#print(num_pers)\n",
        "\n",
        "#NVA = '\\tNovel\\tORG\\tPER\\tGEO\\tGPE\\tTIM\\tART\\tEVE\\tNAT\\n' #\n",
        "#NVA = '\\tNovel\\tNouns\\tVerbs\\tAdjectives\\tUnique_nouns\\tUnique_verbs\\tUnique_adjs\\tNER\\n' #\n",
        "\n",
        "#nom_novela = 'Yeats'\n",
        "#nom_novela = novela_name\n",
        "\n",
        "#novela=novela_text\n",
        "\n",
        "#unique_ent=[]\n",
        "\n",
        "#orgs=[]\n",
        "#pers=[]\n",
        "#geos=[]\n",
        "#geps=[]\n",
        "#tims=[]\n",
        "#arts=[]\n",
        "#eves=[]\n",
        "#nats=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#for ent in analisis.ents: \n",
        " # if ent.ent_type == 'B-geo':\n",
        "  #  eves.append(token.lemma_) #\n",
        " # elif token.pos_ == 'VERB':\n",
        "  #  verbs.append(token.lemma_) #\n",
        "   # if token.lemma not in unique_verb:\n",
        "    #   unique_verb.append(token.lemma)\n",
        " # elif token.pos_ == 'ADJ':\n",
        "  #  adjs.append(token.lemma_) #\n",
        "   # if token.lemma not in unique_adj:\n",
        "    #   unique_adj.append(token.lemma)\n",
        "\n",
        "\n",
        "#num_nouns = str(len(nouns))\n",
        "#num_verbs = str(len(verbs))\n",
        "#num_adjs = str(len(adjs))\n",
        "#num_uninouns = str (len(unique_noun))\n",
        "#num_univerbs = str (len(unique_verb))\n",
        "#num_uniadjs = str (len(unique_adj))\n",
        "#num_eves = str(len(ners))\n",
        "\n",
        "#NVA += '\\t'+nom_novela+'\\t'+num_nouns+'\\t'+num_verbs+'\\t'+num_adjs+'\\t'+num_uninouns+'\\t'+num_univerbs+'\\t'+num_uniadjs+'\\t'+num_eves+'\\n' #\n",
        "\n",
        "#print(NVA)\n",
        "\n",
        "#salida = open('analisis_soloNombres.txt', 'w') #Crea fichero, etc.\n",
        "#salida.write(corpus_soloNombres)\n",
        "#salida.close()\n",
        "#files.download('analisis_soloNombres.txt')\n",
        "\n",
        "#out = open('author_titles.csv', 'w') #Opens a file in write mode (\"w\").\n",
        "#out.write(authors_titles_out) # \"Writes\" the content of authors_titles_out in the file\n",
        "#out.close() #Closes the file\n",
        "#files.download('author_titles.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eden\t2583\t2587\tLOC\n",
            "the south-east\t2795\t2809\tLOC\n",
            "Earth\t8287\t8292\tLOC\n",
            "Griselda\t11026\t11034\tLOC\n",
            "temper—(Lancelot\t11038\t11054\tLOC\n",
            "the\n",
            "     river\t12899\t12913\tLOC\n",
            "Venus\t19796\t19801\tLOC\n",
            "Paradise\t39821\t39829\tLOC\n",
            "Eden\t40861\t40865\tLOC\n",
            "Earth\t45767\t45772\tLOC\n",
            "Plotinus\t57100\t57108\tLOC\n",
            "Avernus\t57483\t57490\tLOC\n",
            "whereof\t68797\t68804\tLOC\n",
            "the distant sea\t78936\t78951\tLOC\n",
            "Venus\t84801\t84806\tLOC\n",
            "Mops\t102146\t102150\tLOC\n",
            "Eden\t104853\t104857\tLOC\n",
            "the\n",
            "     hedge\t119616\t119630\tLOC\n",
            "earth\t128764\t128769\tLOC\n",
            "Earth\t133037\t133042\tLOC\n",
            "Earth\t136921\t136926\tLOC\n",
            "Young England\t146051\t146064\tLOC\n",
            "Peelite\t146729\t146736\tLOC\n",
            "earth\t174724\t174729\tLOC\n",
            "earth\t199919\t199924\tLOC\n",
            "earth\t199951\t199956\tLOC\n",
            "earth\t200786\t200791\tLOC\n",
            "earth\t202027\t202032\tLOC\n",
            "the Cannibal Islands\t208885\t208905\tLOC\n",
            "Cannibal Island\t209133\t209148\tLOC\n",
            "gulf\t267161\t267165\tLOC\n",
            "Mahomet\t285041\t285048\tLOC\n",
            "Apostles\t285702\t285710\tLOC\n",
            "Christian England\t289789\t289806\tLOC\n",
            "the\n",
            "     river\t298868\t298882\tLOC\n",
            "Redruth\t361720\t361727\tLOC\n",
            "earth\t391790\t391795\tLOC\n",
            "Jesuit\t400260\t400266\tLOC\n",
            "the Cannibal Islands\t417851\t417871\tLOC\n",
            "gulf\t422382\t422386\tLOC\n",
            "earth\t435706\t435711\tLOC\n",
            "earth\t443760\t443765\tLOC\n",
            "Eden\t444827\t444831\tLOC\n",
            "East\t454126\t454130\tLOC\n",
            "East\t454257\t454261\tLOC\n",
            "gulf\t478047\t478051\tLOC\n",
            "Europe\t488690\t488696\tLOC\n",
            "the Valley of Vision\t490447\t490467\tLOC\n",
            "Asia\t495353\t495357\tLOC\n",
            "49 entities were found\n"
          ]
        }
      ]
    }
  ]
}
