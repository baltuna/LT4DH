{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baltuna/LT4DH/blob/main/LT4DH_ELTeC_CountingWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9YAgkI8Naco"
      },
      "source": [
        "# Extracting part of speech from ELTeC-ENG\n",
        "\n",
        "Adaptation of a great Colab by Borja Navarro for the LT4DH course in the University of the Basque Country.\n",
        "\n",
        "This version (to be cleaned) uses English resources in contrast to the Spanish one used by Borja Navarro.\n",
        "\n",
        "Original data here:\n",
        "\n",
        "Borja Navarro Colorado | University of Alicante\n",
        "\n",
        "In this case, the information about part of speech has not been manually annotated in the corpus. It is necessary first analyze the novels with a NLP system and then extract the linguistic information. The NLP system used is [SpaCy](https://spacy.io/).\n",
        "\n",
        "The notebook shows:\n",
        "\n",
        "- how to open a novel from ELTeC in COLAB and to analyse it with SpaCy, and\n",
        "- analysing the output of Spacy for DH.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2j6tERTNTcP"
      },
      "source": [
        "## Loading ELTeC-SPA corpus in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kejn4RdANZEy"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget \"https://github.com/COST-ELTeC/ELTeC-eng/archive/refs/heads/master.zip\" # paste here corpus url\n",
        "\n",
        "zip_ref = zipfile.ZipFile('master.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts files here (/content/)\n",
        "zip_ref.close() \n",
        "!rm master.zip #Removes ZIP to save space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLkW02KoOIX6"
      },
      "source": [
        "## SpaCy: download and installing\n",
        "\n",
        "[SpaCy](https://spacy.io/) is a NLP system. It analyzes part of speech and lemmas, sintax (dependencies) and named entities. \n",
        "\n",
        "Three steps:\n",
        "\n",
        "1. Import SpaCy to Colab\n",
        "2. Download langauge module (Spanish)\n",
        "3. Activate module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHMbKkERXhI"
      },
      "source": [
        "import spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm #Download English module (the \"small\" module in this case: \"sm\").\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp_eng = en_core_web_sm.load() #Load English analyzer in \"nlp_eng\"."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsk-hIbqRuw5"
      },
      "source": [
        "## Analyzing a novel from ELTeC-SPA\n",
        "\n",
        "Once we have downloaded the corpus and activated SpaCy, let's analyze one novel.\n",
        "\n",
        "First, select from the corpus [ELTeC-SPA](https://github.com/COST-ELTeC/ELTeC-spa/tree/master/level1) a novel and copy the file name. Then paste the name in the variable \"novela_name\". In this example, we will analyze the novel of Gertrudis GÃ³mez de Avellaneda [*Sab*](https://github.com/COST-ELTeC/ELTeC-spa/blob/master/level1/SPA1021_GomezDeAvellaneda_Sab.xml): SPA1021_GomezDeAvellaneda_Sab.xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVJtGl9vRzpq"
      },
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "novela_name = \"ENG18510_Kingsley.xml\" # Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "print('Analyzing PoS and lemmas')\n",
        "analisis = nlp_eng(novela_text) #Here the novel is analyzed with SpaCy. All the analysis is stored in \"analisis\" variable.\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_13VKGg9A7Z"
      },
      "source": [
        "Now all the analysis is stored in \"analisis\" variable. It only remains to iterate over the variable and extract the information: in this case, part of speech. How to extract information about syntax, named entities, etc. see [SpaCy 101](https://spacy.io/usage/spacy-101)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Zqf9ZC6-0N"
      },
      "source": [
        "NVA = '\\tNovel\\tNouns\\tVerbs\\tAdjectives\\tUnique_nouns\\tUnique_verbs\\tUnique_adjs\\n' #\n",
        "\n",
        "nom_novela = 'Yeats'\n",
        "#nom_novela = novela_name\n",
        "\n",
        "nouns=[]\n",
        "verbs=[]\n",
        "adjs=[]\n",
        "\n",
        "noun_counts= dict()\n",
        "verb_counts= dict()\n",
        "adj_counts= dict()\n",
        "\n",
        "# for token in analisis: \n",
        "#   if token.pos_ == 'NOUN':\n",
        "#     if token.text.lower() in noun_counts:\n",
        "#        noun_counts[token.text.lower()] += 1\n",
        "#     else:\n",
        "#        noun_counts[token.text.lower()] = 1\n",
        "#   elif token.pos_ == 'VERB':\n",
        "#     if token.text.lower() in verb_counts:\n",
        "#        verb_counts[token.text.lower()] += 1\n",
        "#     else:\n",
        "#         verb_counts[token.text.lower()] = 1\n",
        "#   elif token.pos_ == 'ADJ':\n",
        "#     if token.text.lower() in adj_counts:\n",
        "#        adj_counts[token.text.lower()] += 1\n",
        "#     else:\n",
        "#        adj_counts[token.text.lower()] = 1\n",
        "\n",
        "for token in analisis: \n",
        "  if token.pos_ == 'NOUN':\n",
        "    if token.lemma_ in noun_counts:\n",
        "       noun_counts[token.lemma_] += 1\n",
        "    else:\n",
        "       noun_counts[token.lemma_] = 1\n",
        "  elif token.pos_ == 'VERB':\n",
        "    if token.lemma_ in verb_counts:\n",
        "       verb_counts[token.lemma_] += 1\n",
        "    else:\n",
        "        verb_counts[token.lemma_] = 1\n",
        "  elif token.pos_ == 'ADJ':\n",
        "    if token.lemma_ in adj_counts:\n",
        "       adj_counts[token.lemma_] += 1\n",
        "    else:\n",
        "       adj_counts[token.lemma_] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sort the noun_counts dictionary by appearance count in descending order\n",
        "sorted_nouns = sorted(noun_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_verbs = sorted(verb_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_adjs = sorted(adj_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted nouns and their appearance counts\n",
        "for i, (noun, count) in enumerate(sorted_nouns):\n",
        "    if i >= 50:\n",
        "        break\n",
        "    print(noun, count)\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "for i, (verb, count) in enumerate(sorted_verbs):\n",
        "    if i >= 50:\n",
        "        break\n",
        "    print(verb, count)\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "for i, (adj, count) in enumerate(sorted_adjs):\n",
        "    if i >= 50:\n",
        "        break\n",
        "    print(adj, count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}